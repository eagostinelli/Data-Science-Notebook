{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Coursera) - Andrew Ng - Notes \n",
    "\n",
    "## Introduction (week 1)\n",
    "### ML Definition \n",
    "Arthur Samuel (1959) : field of Study that gives computers the ability to learn without being explicitly programmed.\n",
    "Tom Mitchell (1998): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. \n",
    "\n",
    "Diagram: \n",
    "<img src=\"MLDiagram.jpeg\" style=\"height:300px\">\n",
    "\n",
    "### Cost Function \n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "<img src=\"cost_function.png\" style=\"height:300px\">\n",
    "\n",
    "Intuition: h(theta)-> hypothesis and J(theta)-> cost function. For every H you will have a value of J and you need to minimize this last value. Visually if you have 2 parameters thetas then you can get contours: \n",
    "<img src=\"intuition_cost.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "### Gradient Descent \n",
    "We put theta_0 on the x axis and theta_1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n",
    "<img src=\"gradient1.png\" style=\"height:300px\">\n",
    "\n",
    "The learning rate alpha is going to give you the speed to reach the local/global minimum. \n",
    "<img src=\"gradient2.png\" style=\"height:300px\">\n",
    "The assumption is that the partial derivative will decrease in time so even if the learning rate is fixed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression (week 2)\n",
    "### Gradient Descent for Linear Regression \n",
    "This method looks at every example in the entire training set on every step, and is called *batch gradient descent*. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; (J is a quadratic convex function)\n",
    "<img src=\"gradient_regression.png\" style=\"width:450px\">\n",
    "\n",
    "### Gradient Descent for Multiple Linear Regression \n",
    "we just have to repeat it for our 'n' features:\n",
    "<img src=\"gradient_regression2.png\" style=\"width:450px\">\n",
    "\n",
    "### Feature Scaling: to help Gradient Descent \n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because theta will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "Two techniques to help with this are feature scaling and mean normalization. \n",
    "\n",
    "- **Feature scaling** involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "- **Mean normalization** involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice: \n",
    "- **Debugging gradient descent** Make a plot with number of iterations on the x-axis. Now plot the cost function, J(theta) over the number of iterations of gradient descent. If J(theta) ever increases, then you probably need to decrease alpha.\n",
    "- **Automatic convergence test** Declare convergence if J(theta) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it's difficult to choose this threshold value.\n",
    "\n",
    "Rules: \n",
    "if alpha is too small: slow convergence \n",
    "if alpha is too large: may not decrease on every iteration and thus may not converge.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Normal Equation \n",
    "Gradient descent gives one way of minimizing J. Let's discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the \"Normal Equation\" method (classic method learned in Uni!), we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n",
    "<img src=\"normal_equation.png\" style=\"width:450px\">\n",
    "\n",
    "So when is better to use Nromal Equation or Gradient Descent? \n",
    "<img src=\"normal_equation2.png\" style=\"width:650px\">\n",
    "\n",
    "If XT^X is non-invertible, the common causes might be having :\n",
    "- Redundant features, where two features are very closely related (i.e. they are linearly dependent)\n",
    "- Too many features (e.g. m ≤ n). In this case, delete some features or use \"regularization\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (week 3) \n",
    "The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. \n",
    "\n",
    "\n",
    "## Logistic Regression \n",
    "### Hypothesis Representation \n",
    "<img src=\"logistic1.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function \n",
    "We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima.\n",
    "<img src=\"logistic2.png\" style=\"width:450px\">\n",
    "Note that writing the cost function in this way guarantees that J(theta) is convex for logistic regression.\n",
    "<img src=\"logistic3.png\" style=\"width:550px\">\n",
    "\n",
    "Vectorized implementation: \n",
    "<img src=\"logistic4_vec.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Descent \n",
    "this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta. \n",
    "\n",
    "<img src=\"logistic5.png\" style=\"width:450px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation: \n",
    "<img src=\"logistic6.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimization (not only gradient decent)\n",
    "\"Conjugate gradient\", \"BFGS\", and \"L-BFGS\" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they're already tested and highly optimized. Octave provides them. In the course we use the function \"fminunc()\" our cost function, our initial vector of theta values, and the \"options\" object that we created beforehand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification - One vs all strategy \n",
    "We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n",
    "<img src=\"multiclass1.png\" style=\"width:450px\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Regularization \n",
    "### Overfitting \n",
    "we’ll say the figure on the left shows an instance of underfitting—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of overfitting.\n",
    "<img src=\"overfitting.png\" style=\"width:550px\">\n",
    "- **Underfitting, or high bias**, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. \n",
    "- **Overfitting, or high variance**, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to deal with Overfitting? \n",
    "1) Reduce the number of features:\n",
    "\n",
    "    - Manually select which features to keep.\n",
    "    - Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2) Regularization\n",
    "\n",
    "    - Keep all the features, but reduce the magnitude of parameters theta_j.\n",
    "    - Regularization works well when we have a lot of slightly useful features.\n",
    "    \n",
    "### How the cost function will change? \n",
    "The λ, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated. This means: \n",
    "- if lambda is too large, thetas will be so small that you risk to smooth out the model too much - underfitting. \n",
    "- if lambda is too small (λ=0), then the cost function is the same as before, risking not to tackle the overfitting problem. \n",
    "<img src=\"reg1.png\" style=\"width:450px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized linear regression  \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"reg2.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"reg3.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Neural Networks \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
