{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Coursera) - Andrew Ng - Notes \n",
    "\n",
    "## Introduction (week 1)\n",
    "### ML Definition \n",
    "Arthur Samuel (1959) : field of Study that gives computers the ability to learn without being explicitly programmed.\n",
    "Tom Mitchell (1998): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. \n",
    "\n",
    "Diagram: \n",
    "<img src=\"images\\MLDiagram.jpeg\" style=\"width:400px\">\n",
    "\n",
    "### Cost Function \n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "<img src=\"images\\cost_function.png\" style=\"width:500px\">\n",
    "\n",
    "\n",
    "Intuition: h(theta)-> hypothesis and J(theta)-> cost function. For every H you will have a value of J and you need to minimize this last value. Visually if you have 2 parameters thetas then you can get contours: \n",
    "<img src=\"images\\intuition_cost.png\" style=\"width:500px\">\n",
    "\n",
    "\n",
    "### Gradient Descent \n",
    "We put theta_0 on the x axis and theta_1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n",
    "<img src=\"images/gradient1.png\" style=\"width:500px\">\n",
    "\n",
    "The learning rate alpha is going to give you the speed to reach the local/global minimum. \n",
    "<img src=\"images/gradient2.png\" style=\"width:500px\">\n",
    "The assumption is that the partial derivative will decrease in time so even if the learning rate is fixed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression (week 2)\n",
    "### Gradient Descent for Linear Regression \n",
    "This method looks at every example in the entire training set on every step, and is called *batch gradient descent*. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; (J is a quadratic convex function)\n",
    "<img src=\"images/gradient_regression.png\" style=\"width:450px\">\n",
    "\n",
    "### Gradient Descent for Multiple Linear Regression \n",
    "we just have to repeat it for our 'n' features:\n",
    "<img src=\"images/gradient_regression2.png\" style=\"width:450px\">\n",
    "\n",
    "### Feature Scaling: to help Gradient Descent \n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because theta will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "Two techniques to help with this are feature scaling and mean normalization. \n",
    "\n",
    "- **Feature scaling**: involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "- **Mean normalization**: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice: \n",
    "- **Debugging gradient descent** Make a plot with number of iterations on the x-axis. Now plot the cost function, J(theta) over the number of iterations of gradient descent. If J(theta) ever increases, then you probably need to decrease alpha.\n",
    "- **Automatic convergence test** Declare convergence if J(theta) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it's difficult to choose this threshold value.\n",
    "\n",
    "Rules: \n",
    "if alpha is too small: slow convergence \n",
    "if alpha is too large: may not decrease on every iteration and thus may not converge.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Normal Equation \n",
    "Gradient descent gives one way of minimizing J. Let's discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the \"Normal Equation\" method (classic method learned in Uni!), we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n",
    "<img src=\"images/normal_equation.png\" style=\"width:450px\">\n",
    "\n",
    "So when is better to use Nromal Equation or Gradient Descent? \n",
    "<img src=\"images/normal_equation2.png\" style=\"width:650px\">\n",
    "\n",
    "If XT^X is non-invertible, the common causes might be having :\n",
    "- Redundant features, where two features are very closely related (i.e. they are linearly dependent)\n",
    "- Too many features (e.g. m ≤ n). In this case, delete some features or use \"regularization\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (week 3) \n",
    "The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. \n",
    "\n",
    "\n",
    "## Logistic Regression \n",
    "### Hypothesis Representation \n",
    "<img src=\"images/logistic1.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function \n",
    "We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima.\n",
    "<img src=\"images/logistic2.png\" style=\"width:450px\">\n",
    "Note that writing the cost function in this way guarantees that J(theta) is convex for logistic regression.\n",
    "<img src=\"images/logistic3.png\" style=\"width:550px\">\n",
    "\n",
    "Vectorized implementation: \n",
    "<img src=\"images/logistic4_vec.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Descent \n",
    "this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta. \n",
    "\n",
    "<img src=\"images/logistic5.png\" style=\"width:450px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation: \n",
    "<img src=\"images/logistic6.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimization (not only gradient decent)\n",
    "\"Conjugate gradient\", \"BFGS\", and \"L-BFGS\" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they're already tested and highly optimized. Octave provides them. In the course we use the function \"fminunc()\" our cost function, our initial vector of theta values, and the \"options\" object that we created beforehand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification - One vs all strategy \n",
    "We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n",
    "<img src=\"images/multiclass1.png\" style=\"width:450px\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Regularization \n",
    "### Overfitting \n",
    "we’ll say the figure on the left shows an instance of underfitting—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of overfitting.\n",
    "<img src=\"images/overfitting.png\" style=\"width:550px\">\n",
    "- **Underfitting, or high bias**, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. \n",
    "- **Overfitting, or high variance**, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to deal with Overfitting? \n",
    "1) Reduce the number of features:\n",
    "\n",
    "    - Manually select which features to keep.\n",
    "    - Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2) Regularization\n",
    "\n",
    "    - Keep all the features, but reduce the magnitude of parameters theta_j.\n",
    "    - Regularization works well when we have a lot of slightly useful features.\n",
    "    \n",
    "### How the cost function will change? \n",
    "The λ, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated. This means: \n",
    "- if lambda is too large, thetas will be so small that you risk to smooth out the model too much - underfitting. \n",
    "- if lambda is too small (λ=0), then the cost function is the same as before, risking not to tackle the overfitting problem. \n",
    "<img src=\"images/reg1.png\" style=\"width:450px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized linear regression  \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"images/reg2.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"images/reg3.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Neural Networks \n",
    "<img src=\"images/nn1.jpeg\" style=\"width:700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. x_0 : intercept, bias Unit \n",
    "2. a1 : your hidden layer 1 - you calculate it by using the hypothesis (sigmoid in our case, and using the input of the previous layer (a0=x, remember to add the BIAS UNIT!) and the matrix of weight Theta(at the beginning its been given by the course)) - this layer is going to be the input for the next layer (a2 hidden layer two, if you have any). --> a = g(z)\n",
    "3. be careful with the dimension of the your matrix Theta \n",
    "4. How to calculate z of the next layer? z_j = Theta_(j-1) * a_(j-1)\n",
    "\n",
    "<img src=\"images/nn5.png\" style=\"width:500px\">\n",
    "\n",
    "5. Last step: h(x) = a_(j+1) = g(z_(j+1)) - you need to have a Theta that has only one row so that multiplied by column a_j will give you a single number (in this case) \n",
    "\n",
    "\n",
    "Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example of Multiclass classification \n",
    "To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly: \n",
    "<img src=\"images/nn2.png\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cost Function\n",
    "<img src=\"images/nn3.png\" style=\"width:800px\">\n",
    "\n",
    "We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.\n",
    "\n",
    "In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.\n",
    "\n",
    "- the double sum simply adds up the logistic regression costs calculated for each cell in the output layer\n",
    "- the triple sum simply adds up the squares of all the individual Θs in the entire network.\n",
    "- the i in the triple sum does not refer to training example i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation \n",
    "The Back propagation is the same thing that we were doing when performing the gradient descent in a linear regression: we want to minimize the cost function! \n",
    "The idea behind is that you would do normally the forward propagation as usual, and the you would use your actual y to check the output of the neural network - based on that you are going to calculate backwards the differences that every single layer is calculating and our aim is to minimize this difference. \n",
    "\n",
    "This time we are going from the right to the left and accumulating all these differences in D. and Delta (upper case) is the matrix of deltas (lower case) - same concepts as Theta matrix. \n",
    "\n",
    "<img src=\"images/nn4.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the parameter: \n",
    "\n",
    "<img src=\"images/nn6.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking \n",
    "Gradient checking will assure that our backpropagation works as intended. A small value for epsilon such as ϵ=10^−4, guarantees that the math works out properly. If the value for epsilon is too small, we can end up with numerical problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector. Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again. The code to compute gradApprox can be very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initialize the Theta? Random Initialization \n",
    "Don't initialise with Zeros because when we backpropagate, all nodes will update to the same value repeatedly.Instead we can randomly initialize our weights for our Theta matrices using the following method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## E.g. \n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the epsilon used above is unrelated to the epsilon from Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide for Neural Network\n",
    "\n",
    "### Choose the Architecture: \n",
    "- Number of **input units** = dimension of features x^(i)\n",
    "- Number of **output units** = number of classes\n",
    "- Number of **hidden units** per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)\n",
    "- Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.\n",
    "\n",
    "### How to train a NN \n",
    "1. Randomly initialize the weights - Theta matrix \n",
    "2. Implement forward propagation to get h(xi) -> for every xi\n",
    "3. Implement the cost function (take into account the regularization)\n",
    "4. Implement backpropagation to compute partial derivatives (collect info on delta, Delta and D matrices)\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking. - one time is fine, just to check that GradApprox is similar to DeltaVector. \n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n",
    "And for now, we are doing this for every x(i), y(i) -> meaning that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i = 1:m,\n",
    "   Perform forward propagation and backpropagation using example (x(i),y(i))\n",
    "   (Get activations a(l) and delta terms d(l) for l = 2,...,L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Methods to apply ML \n",
    "## Evaluating a Learning Algorithm \n",
    "Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. \n",
    "\n",
    "Given many models with different polynomial degrees, we can use a systematic approach to identify the 'best' function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.\n",
    "\n",
    "One way to break down our dataset into the three sets is:\n",
    "- Training set: 60%\n",
    "- Cross validation set: 20%\n",
    "- Test set: 20%\n",
    "\n",
    "and by using these, we can calculate 3 different types of measurement: \n",
    "- Training set: Optimize the parameters in Theta using the training set for each polynomial degree.\n",
    "- Cross Validation: Find the polynomial degree d with the least error using the cross validation set.\n",
    "- Test set: Estimate the generalization error using the test set with J test. \n",
    "\n",
    "\n",
    "## Bias and Variance \n",
    "We need to distinguish whether bias or variance is the problem contributing to bad predictions.\n",
    "<img src=\"images/advice1.jpeg\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what should we do in order to pick the right Polynomial degree (d): one way is to use the regularization. \n",
    "The lambda is responsible for the weights of theta in the model. \n",
    "\n",
    "- **Bigger the lambda** is, and more 'unforgiving' the calculation of the cost function will be, and the risk is that we would penalise some parameter pushing the theta to be very small in order to minise the cost function the risk related to this is that we would under-fit the model (simplify the model too much). \n",
    "\n",
    "- **Smaller lambda** is more 'forgiving' so we can also have various theta in the model also with high value. The risk is that we would allow some parameters to stay in the model even if there are not of real usage. Risk of overfitting. \n",
    "\n",
    "- if **lambda = zero** then basically you dont have a regularization, so there is still a possible risk of overfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the right value of lambda? \n",
    "\n",
    "1. Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});\n",
    "2. Create a set of models with different degrees or any other variants.\n",
    "3. Iterate through the λs and for each λ go through all the models to learn some Θ. \n",
    "4. Compute the cross validation error using the learned Θ (computed with λ) on the JCV(Θ) without regularization or λ = 0.\n",
    "5. Select the best combo that produces the lowest error on the cross validation set.\n",
    "6. Using the best combo Θ and λ, apply it on Jtest(Θ) to see if it has a good generalization of the problem.\n",
    "\n",
    "\n",
    "<img src=\"images/advice2.jpeg\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves \n",
    "At this point, how should we visualise and interpret the errors of each sets? \n",
    "\n",
    "### Experiencing High Bias: \n",
    "<img src=\"images/advice3.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiencing High Variance: \n",
    "<img src=\"images/advice4.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what to do next? \n",
    "After anlalysing your errors, you should know by now, if your model suffers of High Bias or High Variance problem and based on that we need to take action: \n",
    "\n",
    "- **Getting more training examples**: Fixes high variance\n",
    "- **Trying smaller sets of features**: Fixes high variance\n",
    "- **Adding features**: Fixes high bias\n",
    "- **Adding polynomial features**: Fixes high bias\n",
    "- **Decreasing λ**: Fixes high bias\n",
    "- **Increasing λ**: Fixes high variance.\n",
    "\n",
    "In NN:\n",
    "- A neural network with **fewer parameters** is prone to **underfitting**. It is also computationally cheaper.\n",
    "- A large neural network with **more parameters** is prone to **overfitting**. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting.\n",
    "\n",
    "Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
