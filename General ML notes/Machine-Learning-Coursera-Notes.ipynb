{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Coursera) - Andrew Ng - Notes \n",
    "\n",
    "## Introduction (week 1)\n",
    "### ML Definition \n",
    "Arthur Samuel (1959) : field of Study that gives computers the ability to learn without being explicitly programmed.\n",
    "Tom Mitchell (1998): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. \n",
    "\n",
    "Diagram: \n",
    "<img src=\"images\\MLDiagram.jpeg\" style=\"width:400px\">\n",
    "\n",
    "### Cost Function \n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "<img src=\"images\\cost_function.png\" style=\"width:500px\">\n",
    "\n",
    "\n",
    "Intuition: h(theta)-> hypothesis and J(theta)-> cost function. For every H you will have a value of J and you need to minimize this last value. Visually if you have 2 parameters thetas then you can get contours: \n",
    "<img src=\"images\\intuition_cost.png\" style=\"width:500px\">\n",
    "\n",
    "\n",
    "### Gradient Descent \n",
    "We put theta_0 on the x axis and theta_1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n",
    "<img src=\"images/gradient1.png\" style=\"width:500px\">\n",
    "\n",
    "The learning rate alpha is going to give you the speed to reach the local/global minimum. \n",
    "<img src=\"images/gradient2.png\" style=\"width:500px\">\n",
    "The assumption is that the partial derivative will decrease in time so even if the learning rate is fixed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression (week 2)\n",
    "### Gradient Descent for Linear Regression \n",
    "This method looks at every example in the entire training set on every step, and is called *batch gradient descent*. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; (J is a quadratic convex function)\n",
    "<img src=\"images/gradient_regression.png\" style=\"width:450px\">\n",
    "\n",
    "### Gradient Descent for Multiple Linear Regression \n",
    "we just have to repeat it for our 'n' features:\n",
    "<img src=\"images/gradient_regression2.png\" style=\"width:450px\">\n",
    "\n",
    "### Feature Scaling: to help Gradient Descent \n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because theta will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "Two techniques to help with this are feature scaling and mean normalization. \n",
    "\n",
    "- **Feature scaling**: involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "- **Mean normalization**: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice: \n",
    "- **Debugging gradient descent** Make a plot with number of iterations on the x-axis. Now plot the cost function, J(theta) over the number of iterations of gradient descent. If J(theta) ever increases, then you probably need to decrease alpha.\n",
    "- **Automatic convergence test** Declare convergence if J(theta) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it's difficult to choose this threshold value.\n",
    "\n",
    "Rules: \n",
    "if alpha is too small: slow convergence \n",
    "if alpha is too large: may not decrease on every iteration and thus may not converge.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Normal Equation \n",
    "Gradient descent gives one way of minimizing J. Let's discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the \"Normal Equation\" method (classic method learned in Uni!), we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n",
    "<img src=\"images/normal_equation.png\" style=\"width:450px\">\n",
    "\n",
    "So when is better to use Nromal Equation or Gradient Descent? \n",
    "<img src=\"images/normal_equation2.png\" style=\"width:650px\">\n",
    "\n",
    "If XT^X is non-invertible, the common causes might be having :\n",
    "- Redundant features, where two features are very closely related (i.e. they are linearly dependent)\n",
    "- Too many features (e.g. m ≤ n). In this case, delete some features or use \"regularization\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (week 3) \n",
    "The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. \n",
    "\n",
    "\n",
    "## Logistic Regression \n",
    "### Hypothesis Representation \n",
    "<img src=\"images/logistic1.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function \n",
    "We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima.\n",
    "<img src=\"images/logistic2.png\" style=\"width:450px\">\n",
    "Note that writing the cost function in this way guarantees that J(theta) is convex for logistic regression.\n",
    "<img src=\"images/logistic3.png\" style=\"width:550px\">\n",
    "\n",
    "Vectorized implementation: \n",
    "<img src=\"images/logistic4_vec.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Descent \n",
    "this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta. \n",
    "\n",
    "<img src=\"images/logistic5.png\" style=\"width:450px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation: \n",
    "<img src=\"images/logistic6.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimization (not only gradient decent)\n",
    "\"Conjugate gradient\", \"BFGS\", and \"L-BFGS\" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they're already tested and highly optimized. Octave provides them. In the course we use the function \"fminunc()\" our cost function, our initial vector of theta values, and the \"options\" object that we created beforehand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification - One vs all strategy \n",
    "We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n",
    "<img src=\"images/multiclass1.png\" style=\"width:450px\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Regularization \n",
    "### Overfitting \n",
    "we’ll say the figure on the left shows an instance of underfitting—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of overfitting.\n",
    "<img src=\"images/overfitting.png\" style=\"width:550px\">\n",
    "- **Underfitting, or high bias**, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. \n",
    "- **Overfitting, or high variance**, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to deal with Overfitting? \n",
    "1) Reduce the number of features:\n",
    "\n",
    "    - Manually select which features to keep.\n",
    "    - Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2) Regularization\n",
    "\n",
    "    - Keep all the features, but reduce the magnitude of parameters theta_j.\n",
    "    - Regularization works well when we have a lot of slightly useful features.\n",
    "    \n",
    "### How the cost function will change? \n",
    "The λ, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated. This means: \n",
    "- if lambda is too large, thetas will be so small that you risk to smooth out the model too much - underfitting. \n",
    "- if lambda is too small (λ=0), then the cost function is the same as before, risking not to tackle the overfitting problem. \n",
    "<img src=\"images/reg1.png\" style=\"width:450px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized linear regression  \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"images/reg2.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression \n",
    "Remember to exclude theta_0 because we don't need to regularize the intercept. \n",
    "<img src=\"images/reg3.png\" style=\"width:650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Neural Networks \n",
    "<img src=\"images/nn1.jpeg\" style=\"width:700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. x_0 : intercept, bias Unit \n",
    "2. a1 : your hidden layer 1 - you calculate it by using the hypothesis (sigmoid in our case, and using the input of the previous layer (a0=x, remember to add the BIAS UNIT!) and the matrix of weight Theta(at the beginning its been given by the course)) - this layer is going to be the input for the next layer (a2 hidden layer two, if you have any). --> a = g(z)\n",
    "3. be careful with the dimension of the your matrix Theta \n",
    "4. How to calculate z of the next layer? z_j = Theta_(j-1) * a_(j-1)\n",
    "\n",
    "<img src=\"images/nn5.png\" style=\"width:500px\">\n",
    "\n",
    "5. Last step: h(x) = a_(j+1) = g(z_(j+1)) - you need to have a Theta that has only one row so that multiplied by column a_j will give you a single number (in this case) \n",
    "\n",
    "\n",
    "Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example of Multiclass classification \n",
    "To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly: \n",
    "<img src=\"images/nn2.png\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cost Function\n",
    "<img src=\"images/nn3.png\" style=\"width:800px\">\n",
    "\n",
    "We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.\n",
    "\n",
    "In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.\n",
    "\n",
    "- the double sum simply adds up the logistic regression costs calculated for each cell in the output layer\n",
    "- the triple sum simply adds up the squares of all the individual Θs in the entire network.\n",
    "- the i in the triple sum does not refer to training example i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation \n",
    "The Back propagation is the same thing that we were doing when performing the gradient descent in a linear regression: we want to minimize the cost function! \n",
    "The idea behind is that you would do normally the forward propagation as usual, and the you would use your actual y to check the output of the neural network - based on that you are going to calculate backwards the differences that every single layer is calculating and our aim is to minimize this difference. \n",
    "\n",
    "This time we are going from the right to the left and accumulating all these differences in D. and Delta (upper case) is the matrix of deltas (lower case) - same concepts as Theta matrix. \n",
    "\n",
    "<img src=\"images/nn4.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the parameter: \n",
    "\n",
    "<img src=\"images/nn6.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking \n",
    "Gradient checking will assure that our backpropagation works as intended. A small value for epsilon such as ϵ=10^−4, guarantees that the math works out properly. If the value for epsilon is too small, we can end up with numerical problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector. Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again. The code to compute gradApprox can be very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initialize the Theta? Random Initialization \n",
    "Don't initialise with Zeros because when we backpropagate, all nodes will update to the same value repeatedly.Instead we can randomly initialize our weights for our Theta matrices using the following method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## E.g. \n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the epsilon used above is unrelated to the epsilon from Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide for Neural Network\n",
    "\n",
    "### Choose the Architecture: \n",
    "- Number of **input units** = dimension of features x^(i)\n",
    "- Number of **output units** = number of classes\n",
    "- Number of **hidden units** per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)\n",
    "- Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.\n",
    "\n",
    "### How to train a NN \n",
    "1. Randomly initialize the weights - Theta matrix \n",
    "2. Implement forward propagation to get h(xi) -> for every xi\n",
    "3. Implement the cost function (take into account the regularization)\n",
    "4. Implement backpropagation to compute partial derivatives (collect info on delta, Delta and D matrices)\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking. - one time is fine, just to check that GradApprox is similar to DeltaVector. \n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n",
    "And for now, we are doing this for every x(i), y(i) -> meaning that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i = 1:m,\n",
    "   Perform forward propagation and backpropagation using example (x(i),y(i))\n",
    "   (Get activations a(l) and delta terms d(l) for l = 2,...,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
